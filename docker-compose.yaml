# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#

version: '3'

x-aliases:
  - &airflow_build_params
      build:
        context: .
        dockerfile: Dockerfile
        args:
          conda: "${conda:-true}"

  - &airflow_env
      environment:
        AIRFLOW__CORE__EXECUTOR: LocalExecutor
        AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${POSTGRE_USER:-airflow}:${POSTGRE_PASS:-airflow}@${POSTGRE_SERVER:-postgres}/${POSTGRE_DB:-airflow}
        AIRFLOW__CORE__FERNET_KEY:
        AIRFLOW_CONN_METADATA_DB: postgresql+psycopg2://${POSTGRE_USER:-airflow}:${POSTGRE_PASS:-airflow}@${POSTGRE_SERVER:-postgres}/${POSTGRE_DB:-airflow}
        AIRFLOW_VAR__METADATA_DB_SCHEMA: ${POSTGRE_DB:-airflow}
        AIRFLOW__SCHEDULER__SCHEDULER_HEARTBEAT_SEC: 10
        AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: "True"
        AIRFLOW__CORE__LOAD_EXAMPLES: ${AIRFLOW__CORE__LOAD_EXAMPLES:-False}
        AIRFLOW__CORE__DAGS_FOLDER: ${DAGS_FOLDER:-/opt/airflow/dags}
        _AIRFLOW_DB_UPGRADE: "True"
        _AIRFLOW_WWW_USER_CREATE: "True"
        AIRFLOW__WEBSERVER__ENABLE_PROXY_FIX: "True"
        _AIRFLOW_WWW_USER_USERNAME: ${_AIRFLOW_WWW_USER_USERNAME:-airflow}
        _AIRFLOW_WWW_USER_PASSWORD: ${_AIRFLOW_WWW_USER_PASSWORD:-airflow}
        AIRFLOW__WEBSERVER__BASE_URL: ${BASE_URL:-http://localhost:8080}
        AIRFLOW__LOGGING__BASE_LOG_FOLDER: "/opt/airflow/logs"
        PROJECT_DIR: "/opt/airflow/project"
        conda: "${conda:-true}"
        

  - &airflow_volumes
      volumes:
        - ${PROJECT_DIR:-./project}:/opt/airflow/project
        - ${DAGS_DIR:-./dags}:/opt/airflow/dags
        - ${LOGS_DIR:-./airflow-logs}:/opt/airflow/logs
        - ${CWL_TMP_FOLDER:-./cwl_tmp_folder}:/opt/airflow/project/temp/cwl_tmp_folder
        - ${CWL_INPUTS_FOLDER:-./cwl_inputs_folder}:/opt/airflow/project/temp/cwl_inputs_folder
        - ${CWL_OUTPUTS_FOLDER:-./cwl_outputs_folder}:/opt/airflow/project/temp/cwl_outputs_folder
        - ${CWL_PICKLE_FOLDER:-./cwl_pickle_folder}:/opt/airflow/project/temp/cwl_pickle_folder
        - type: bind
          source: /var/run/docker.sock
          target: /var/run/docker.sock
      



services:
  scheduler:
    image: ${AIRFLOW_IMAGE_NAME:-myairflow-conda}
    <<: *airflow_build_params
    restart: always
    user: root
    container_name: scheduler
    depends_on:
      - webserver
    <<: *airflow_env
    <<: *airflow_volumes
    command: [ "airflow", "scheduler" ]
    network_mode: bridge
  
  webserver:
    image: ${AIRFLOW_IMAGE_NAME:-myairflow-conda}
    <<: *airflow_build_params
    container_name: webserver
    restart: always
    user: root
    <<: *airflow_env
    <<: *airflow_volumes
    ports:
      - "${WEB_SERVER_PORT:-8080}:8080"
    command: [ "airflow", "webserver" ]
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
      interval: 10s
      timeout: 10s
      retries: 5
    network_mode: bridge
